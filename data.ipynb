{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f1051d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hncha\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cb25e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thiết lập hoàn tất. Chuyển sang Cell 2.\n"
     ]
    }
   ],
   "source": [
    "# Cài đặt Thư viện và Khai báo Hàm Đọc Dữ liệu (Cell 1 Đã Cập nhật)\n",
    "\n",
    "# ... (Giữ nguyên các import) ...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import io\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "\n",
    "# --- CẤU HÌNH ---\n",
    "FILE_PATHS = [\n",
    "    'dict_normal.csv',\n",
    "    'dict_special.csv',\n",
    "    'dict_general.csv'\n",
    "  \n",
    "]\n",
    "TRAIN_FILE = 'data_train.csv'\n",
    "TARGET_STD_COLS = ['abb', 'meaning']\n",
    "\n",
    "# **ĐÃ CẬP NHẬT:** Kiểm tra lại tất cả các delimiter phổ biến (ưu tiên dấu phẩy)\n",
    "DELIMITERS = [',', ';', '\\t', '|'] \n",
    "ENCODINGS = ['utf-8', 'latin-1', 'cp1252']\n",
    "\n",
    "# --- HÀM ĐỌC FILE RẤT BỀN BỈ (ĐÃ FIX LỖI DELIMITER) ---\n",
    "def robust_read_file_content(file):\n",
    "    \"\"\"Đọc file bằng Python native, thử các delimiter phổ biến.\"\"\"\n",
    "    raw_content = None\n",
    "    \n",
    "    # BƯỚC 1: Đọc nội dung thô (raw content)\n",
    "    for enc in ENCODINGS:\n",
    "        try:\n",
    "            with open(file, 'r', encoding=enc) as f:\n",
    "                raw_content = f.read()\n",
    "                break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Không tìm thấy file: {file}. Vui lòng kiểm tra lại tên file và thư mục.\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if raw_content is None:\n",
    "        raise ValueError(f\"Không thể giải mã (decode) nội dung file {file} với bất kỳ encoding nào.\")\n",
    "\n",
    "    # BƯỚC 2: Phân tích nội dung thô bằng thư viện CSV (thử các delimiter)\n",
    "    for sep in DELIMITERS:\n",
    "        try:\n",
    "            csv_reader = csv.reader(io.StringIO(raw_content), delimiter=sep)\n",
    "            all_rows = list(csv_reader)\n",
    "            if not all_rows: continue\n",
    "            \n",
    "            header = all_rows[0]\n",
    "            data = [row for row in all_rows[1:] if len(row) >= 2]\n",
    "            \n",
    "            if len(data) > 10: \n",
    "                print(f\"  -> Đọc thành công với delimiter: '{sep}'\")\n",
    "                return header, data, sep\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "    raise ValueError(f\"Không thể phân tích nội dung file {file} thành 2 cột với các delimiter phổ biến.\")\n",
    "\n",
    "print(\"Thiết lập hoàn tất. Chuyển sang Cell 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1afdac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BƯỚC 1: ĐỌC VÀ HỢP NHẤT DỮ LIỆU TỪ ĐIỂN ---\n",
      "Đang xử lý file: dict_normal.csv...\n",
      "  -> Đọc thành công với delimiter: ';'\n",
      "  -> dict_normal.csv xử lý thành công qua header.\n",
      "Đang xử lý file: dict_special.csv...\n",
      "  -> Đọc thành công với delimiter: ';'\n",
      "  -> dict_special.csv xử lý thành công qua header.\n",
      "Đang xử lý file: dict_general.csv...\n",
      "  -> Đọc thành công với delimiter: ';'\n",
      "  -> dict_general.csv xử lý thành công qua header.\n",
      "\n",
      "--- KẾT QUẢ HỢP NHẤT THÀNH CÔNG ---\n",
      "Tổng số cặp viết tắt/nghĩa sau khi hợp nhất và làm sạch: 1685\n",
      "  abb meaning\n",
      "0   ă       á\n",
      "1   â       á\n",
      "2   ế       á\n",
      "3   í       á\n",
      "4  àh       à\n",
      "\n",
      "✅ Dữ liệu từ điển đã sẵn sàng. Vui lòng chạy Cell 3 để gán nhãn.\n",
      "Đã tạo file master_dict_data.csv thành công.\n"
     ]
    }
   ],
   "source": [
    "all_dictionaries = []\n",
    "\n",
    "print(\"--- BƯỚC 1: ĐỌC VÀ HỢP NHẤT DỮ LIỆU TỪ ĐIỂN ---\")\n",
    "for file in FILE_PATHS:\n",
    "    print(f\"Đang xử lý file: {file}...\")\n",
    "    try:\n",
    "        header, data, sep = robust_read_file_content(file) \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Lấy header đã được dọn dẹp: \n",
    "        # 1. Chuyển thành chữ thường\n",
    "        # 2. Loại bỏ khoảng trắng thừa (.strip())\n",
    "        # 3. LOẠI BỎ KÝ TỰ BOM ('\\ufeff')\n",
    "        header_stripped = [str(c).lower().strip().replace('\\ufeff', '') for c in header]\n",
    "        \n",
    "        found_abb_col_index = None\n",
    "        found_meaning_col_index = None\n",
    "        \n",
    "        # Xử lý các file không có header rõ ràng (như dict_sheet2.csv)\n",
    "        if file == 'dict_sheet2.csv' or all(c == '' for c in header) or len(header_stripped) < 2:\n",
    "            \n",
    "            if df.shape[1] >= 2:\n",
    "                df = df.iloc[:, :2]\n",
    "                df.columns = TARGET_STD_COLS\n",
    "                print(f\"  -> {file} được xử lý bằng cách lấy 2 cột đầu (dạng không header).\")\n",
    "            else:\n",
    "                 print(f\"  [LỖI CẤU TRÚC] File {file} không có đủ 2 cột dữ liệu. Bỏ qua.\")\n",
    "                 continue\n",
    "        else:\n",
    "            # Xử lý các file có header đã được làm sạch\n",
    "            for i, col in enumerate(header_stripped):\n",
    "                # Tên cột chính xác sau khi strip và bỏ BOM: abbreviation hoặc meaning\n",
    "                if col == 'abbreviation':\n",
    "                    found_abb_col_index = i\n",
    "                if col == 'meaning':\n",
    "                    found_meaning_col_index = i\n",
    "            \n",
    "            # Nếu không tìm thấy abbreviation hoặc meaning, thử các tên thay thế\n",
    "            if found_abb_col_index is None or found_meaning_col_index is None:\n",
    "                for i, col in enumerate(header_stripped):\n",
    "                    if col in ['abb', 'viết tắt'] and found_abb_col_index is None:\n",
    "                        found_abb_col_index = i\n",
    "                    if col in ['origin', 'nghĩa'] and found_meaning_col_index is None:\n",
    "                        found_meaning_col_index = i\n",
    "            \n",
    "            \n",
    "            if found_abb_col_index is not None and found_meaning_col_index is not None:\n",
    "                # Nếu tìm thấy, chọn và đổi tên cột\n",
    "                df = df.iloc[:, [found_abb_col_index, found_meaning_col_index]]\n",
    "                df.columns = TARGET_STD_COLS \n",
    "                print(f\"  -> {file} xử lý thành công qua header.\")\n",
    "            else:\n",
    "                print(f\"  [LỖI HEADER] Không tìm thấy cột 'abbreviation'/'meaning' (hoặc tên thay thế) trong header file: {file}. Header được đọc (đã strip & bỏ BOM): {header_stripped}. Bỏ qua.\")\n",
    "                continue\n",
    "\n",
    "        all_dictionaries.append(df) \n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  [LỖI FILE NOT FOUND] KHÔNG TÌM THẤY FILE: {file}. Vui lòng kiểm tra lại tên file và thư mục.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [LỖI CHUNG] Xử lý file {file} thất bại: {e}. Bỏ qua.\")\n",
    "\n",
    "# Hợp nhất và làm sạch dữ liệu\n",
    "if all_dictionaries:\n",
    "    master_dict_df = pd.concat(all_dictionaries, ignore_index=True)\n",
    "    master_dict_df.dropna(subset=['abb', 'meaning'], inplace=True)\n",
    "    master_dict_df['abb'] = master_dict_df['abb'].astype(str).str.lower().str.strip()\n",
    "    master_dict_df['meaning'] = master_dict_df['meaning'].astype(str).str.lower().str.strip()\n",
    "    master_dict_df.drop_duplicates(subset=['abb', 'meaning'], inplace=True)\n",
    "\n",
    "    print(\"\\n--- KẾT QUẢ HỢP NHẤT THÀNH CÔNG ---\")\n",
    "    print(f\"Tổng số cặp viết tắt/nghĩa sau khi hợp nhất và làm sạch: {len(master_dict_df)}\")\n",
    "    print(master_dict_df.head())\n",
    "    print(\"\\n✅ Dữ liệu từ điển đã sẵn sàng. Vui lòng chạy Cell 3 để gán nhãn.\")\n",
    "else:\n",
    "    print(\"\\n[LỖI NẶNG] KHÔNG CÓ DATASET NÀO ĐƯỢC LOAD THÀNH CÔNG.\")\n",
    "    \n",
    "    # CHẠY DÒNG NÀY ĐỂ LƯU TỪ ĐIỂN CẦN THIẾT CHO FLASK\n",
    "master_dict_df.to_csv('master_dict_data.csv', index=False, encoding='utf-8')\n",
    "print(\"Đã tạo file master_dict_data.csv thành công.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a237e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-List (Danh sách từ xúc phạm) đã được tạo.\n",
      "\n",
      "--- BƯỚC 3: GÁN NHÃN TỰ ĐỘNG ---\n",
      "  -> Đọc thành công với delimiter: ';'\n",
      "label\n",
      "0    1004\n",
      "1     213\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Đã lưu tập dữ liệu đã gán nhãn vào: labeled_train_data.csv. Chuyển sang Cell 4.\n"
     ]
    }
   ],
   "source": [
    "# --- BƯỚC 2: XÂY DỰNG P-LIST THỦ CÔNG ---\n",
    "profanity_list = set([\n",
    "    'vãi lồn', 'vãi cả lồn', 'vãi cứt', 'vãi đái', 'vãi l*n', \n",
    "    'địt', 'địt mẹ', 'đéo', 'đĩ mẹ', 'đkm', 'đậu má', 'đm', \n",
    "    'cặc', 'con cặc', 'thằng cặc', 'cứk', 'c*c', 'con đĩ', 'đĩ',\n",
    "    'lồn', 'cái lồn', 'lồn', 'l*n', \n",
    "    'chó má', 'con chó', 'thằng chó', 'chó chết', 'óc chó', 'óc heo'\n",
    "    'súc vật', 'chết tiệt', 'ngu lol', 'ngu vcl', 'quân phản động', 'mất dạy', 'ba que','ngu','đần'\n",
    "])\n",
    "print(\"P-List (Danh sách từ xúc phạm) đã được tạo.\")\n",
    "\n",
    "# --- BƯỚC 3: GÁN NHÃN TỰ ĐỘNG CHO TẬP HUẤN LUYỆN ---\n",
    "print(\"\\n--- BƯỚC 3: GÁN NHÃN TỰ ĐỘNG ---\")\n",
    "\n",
    "# Đọc file train\n",
    "_, train_data, _ = robust_read_file_content(TRAIN_FILE)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "# File train có cấu trúc: abb, start_index, end_index, cmt (cột 3), origin\n",
    "if train_df.shape[1] >= 4:\n",
    "    train_df = train_df.iloc[:, 3]\n",
    "    train_df = pd.DataFrame(train_df)\n",
    "    train_df.columns = ['text']\n",
    "else:\n",
    "    raise ValueError(f\"File {TRAIN_FILE} không có đủ cột dữ liệu (cmt) cần thiết. Dừng chương trình.\")\n",
    "\n",
    "train_df['text'] = train_df['text'].astype(str).str.lower().str.strip()\n",
    "norm_dict = dict(zip(master_dict_df['abb'], master_dict_df['meaning']))\n",
    "\n",
    "def normalize_and_label(text, norm_dict, profanity_list):\n",
    "    \"\"\"Thực hiện chuẩn hóa (de-abbreviate) và gán nhãn.\"\"\"\n",
    "    normalized_text = text\n",
    "    # 1. Chuẩn hóa\n",
    "    for abb, meaning in norm_dict.items():\n",
    "        pattern = r'\\b' + re.escape(abb) + r'\\b'\n",
    "        normalized_text = re.sub(pattern, meaning, normalized_text)\n",
    "        \n",
    "    is_profane = 0\n",
    "    # 2. Gán nhãn\n",
    "    for profane_word in profanity_list:\n",
    "        if profane_word in normalized_text:\n",
    "            is_profane = 1\n",
    "            break\n",
    "    \n",
    "    return normalized_text, is_profane\n",
    "\n",
    "# Áp dụng hàm gán nhãn\n",
    "train_df[['normalized_text', 'label']] = train_df['text'].apply(\n",
    "    lambda x: pd.Series(normalize_and_label(x, norm_dict, profanity_list))\n",
    ")\n",
    "\n",
    "print(train_df['label'].value_counts())\n",
    "train_df.to_csv('labeled_train_data.csv', index=False, encoding='utf-8')\n",
    "print(\"\\nĐã lưu tập dữ liệu đã gán nhãn vào: labeled_train_data.csv. Chuyển sang Cell 4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5da2c096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu huấn luyện mô hình với 1217 mẫu dữ liệu.\n",
      "\n",
      "--- BÁO CÁO ĐÁNH GIÁ MÔ HÌNH ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       201\n",
      "           1       0.73      0.81      0.77        43\n",
      "\n",
      "    accuracy                           0.91       244\n",
      "   macro avg       0.84      0.87      0.86       244\n",
      "weighted avg       0.92      0.91      0.92       244\n",
      "\n",
      "\n",
      "✅ HOÀN THÀNH TẤT CẢ CÁC BƯỚC! Đã lưu mô hình và Vectorizer.\n",
      "Bạn có thể sử dụng profanity_model.pkl và tfidf_vectorizer.pkl để tích hợp API.\n"
     ]
    }
   ],
   "source": [
    "# --- BƯỚC 4: HUẤN LUYỆN MÔ HÌNH PHÂN LOẠI LOGISTIC REGRESSION ---\n",
    "\n",
    "df_labeled = train_df # Sử dụng DataFrame đã được gán nhãn từ Cell 3\n",
    "print(f\"Bắt đầu huấn luyện mô hình với {len(df_labeled)} mẫu dữ liệu.\")\n",
    "\n",
    "# 1. Chia Dữ liệu\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_labeled['normalized_text'], \n",
    "    df_labeled['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df_labeled['label']\n",
    ")\n",
    "\n",
    "# 2. Vector hóa (TF-IDF)\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3), # Sử dụng n-gram từ 1 đến 3\n",
    "    max_features=10000 \n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train.astype(str))\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test.astype(str))\n",
    "\n",
    "# 3. Huấn luyện Mô hình\n",
    "model = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced') \n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 4. Đánh giá Mô hình\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(\"\\n--- BÁO CÁO ĐÁNH GIÁ MÔ HÌNH ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 5. Lưu Mô hình và Vectorizer\n",
    "with open('profanity1_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open('tfidf_vectorizer1.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "print(\"\\n✅ HOÀN THÀNH TẤT CẢ CÁC BƯỚC! Đã lưu mô hình và Vectorizer.\")\n",
    "print(\"Bạn có thể sử dụng profanity_model.pkl và tfidf_vectorizer.pkl để tích hợp API.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bde3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1. Tải dữ liệu đã gán nhãn và làm sạch (từ Cell 3)\n",
    "try:\n",
    "    df = pd.read_csv('labeled_train_data.csv')\n",
    "    df = df.dropna(subset=['normalized_text', 'label'])\n",
    "except FileNotFoundError:\n",
    "    print(\"Vui lòng đảm bảo file labeled_train_data.csv đã được tạo.\")\n",
    "    \n",
    "# Chuyển nhãn về dạng int\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# 2. Tải PhoBERT Tokenizer\n",
    "MODEL_NAME = \"vinai/phobert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 3. Chuẩn bị tập dữ liệu (Chia và Mã hóa)\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "# Chuyển đổi Pandas DataFrame sang Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[['normalized_text', 'label']])\n",
    "eval_dataset = Dataset.from_pandas(eval_df[['normalized_text', 'label']])\n",
    "\n",
    "# Hàm mã hóa (tokenize)\n",
    "def tokenize_function(examples):\n",
    "    # PhoBERT giới hạn ở 256 token, nhưng thường là 256\n",
    "    return tokenizer(examples[\"normalized_text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True).remove_columns([\"normalized_text\", \"__index_level_0__\"])\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True).remove_columns([\"normalized_text\", \"__index_level_0__\"])\n",
    "\n",
    "# Đổi tên cột\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_eval = tokenized_eval.rename_column(\"label\", \"labels\")\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_eval.set_format(\"torch\")\n",
    "\n",
    "\n",
    "# 4. Tải Mô hình và Định cấu hình cho Phân loại Nhị phân (num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "# 5. Định nghĩa các tham số Huấn luyện\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phobert_results\",\n",
    "    num_train_epochs=3,                     # Số lượng epoch\n",
    "    per_device_train_batch_size=16,         # Kích thước batch cho huấn luyện\n",
    "    per_device_eval_batch_size=16,          # Kích thước batch cho đánh giá\n",
    "    warmup_steps=500,                       # Số bước làm nóng\n",
    "    weight_decay=0.01,                      # Giảm trọng số\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",            # Đánh giá sau mỗi epoch\n",
    "    save_strategy=\"epoch\",                  # Lưu mô hình sau mỗi epoch\n",
    "    load_best_model_at_end=True,            # Tải mô hình tốt nhất sau khi kết thúc\n",
    ")\n",
    "\n",
    "# 6. Định nghĩa Hàm Đánh giá (dùng cho Trainer)\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(p.label_ids, preds),\n",
    "        'f1_macro': f1_score(p.label_ids, preds, average='macro'),\n",
    "        'precision_macro': precision_score(p.label_ids, preds, average='macro'),\n",
    "        'recall_macro': recall_score(p.label_ids, preds, average='macro'),\n",
    "    }\n",
    "\n",
    "# 7. Khởi tạo Trainer và Bắt đầu Fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n--- BẮT ĐẦU FINE-TUNING PHOBERT ---\")\n",
    "trainer.train()\n",
    "\n",
    "# Lưu mô hình tốt nhất và tokenizer đã fine-tuned\n",
    "trainer.save_model(\"./final_phobert_model\")\n",
    "tokenizer.save_pretrained(\"./final_phobert_model\")\n",
    "print(\"\\n✅ Fine-tuning hoàn tất. Mô hình đã được lưu vào thư mục: final_phobert_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
